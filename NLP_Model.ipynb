{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoADoJJuWH1I",
        "outputId": "5b19c224-ef4a-4daf-f9b8-68f574f0fae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download the stopwords and wordnet lemmatizer data\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_answers(input_file, output_file):\n",
        "  with open(input_file, 'r') as input_csv, open(output_file, 'w') as output_csv:\n",
        "    reader = csv.reader(input_csv)\n",
        "    writer = csv.writer(output_csv)\n",
        "\n",
        "    # Initialize the WordNet lemmatizer and stop words set\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Iterate over the rows in the input CSV\n",
        "    for row in reader:\n",
        "      question = row[0]\n",
        "      answer = row[1]\n",
        "\n",
        "      # Pre-process the answer by lowercasing it and removing any punctuation\n",
        "      answer = answer.lower()\n",
        "      answer = re.sub(r'[^\\w\\s]', '', answer)\n",
        "\n",
        "      # Tokenize the answer\n",
        "      tokens = nltk.word_tokenize(answer)\n",
        "\n",
        "      # Remove stop words and lemmatize the remaining tokens\n",
        "      tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "      # Join the lemmatized tokens back into a single string\n",
        "      answer = ' '.join(tokens)\n",
        "\n",
        "      # Write the question and pre-processed answer to the output CSV\n",
        "      writer.writerow([question, answer])\n",
        "\n",
        "preprocess_answers('/content/sample_data/fyp.csv', 'output.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "\n",
        "def split_data(input_file, train_output_file, test_output_file, test_size=0.2):\n",
        "  with open(input_file, 'r') as input_csv, \\\n",
        "       open(train_output_file, 'w') as train_csv, \\\n",
        "       open(test_output_file, 'w') as test_csv:\n",
        "    reader = csv.reader(input_csv)\n",
        "    train_writer = csv.writer(train_csv)\n",
        "    test_writer = csv.writer(test_csv)\n",
        "\n",
        "    # Read in all the rows from the input CSV file\n",
        "    rows = [row for row in reader]\n",
        "\n",
        "    # Randomly shuffle the rows\n",
        "    random.shuffle(rows)\n",
        "\n",
        "    # Calculate the size of the test set\n",
        "    test_size = int(len(rows) * test_size)\n",
        "\n",
        "    # Split the rows into the train and test sets\n",
        "    train_rows = rows[test_size:]\n",
        "    test_rows = rows[:test_size]\n",
        "\n",
        "    # Write the train and test rows to the output CSV files\n",
        "    train_writer.writerows(train_rows)\n",
        "    test_writer.writerows(test_rows)\n",
        "\n",
        "split_data('output.csv', 'train.csv', 'test.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mHkkvcwPcFNY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "questions = []\n",
        "answers = []\n",
        "with open('train.csv', 'r') as train_csv:\n",
        "  reader = csv.reader(train_csv)\n",
        "  for row in reader:\n",
        "    questions.append(row[0])\n",
        "    answers.append(row[1])\n",
        "\n",
        "# Preprocess the data\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(questions)\n",
        "y = answers\n",
        "\n",
        "# Train a logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWtER8WEq4_6",
        "outputId": "b59a975b-98b9-4931-de1e-5de3d0be7762"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Read the questions and answers from the train.csv file\n",
        "questions = []\n",
        "answers = []\n",
        "with open('train.csv', 'r') as train_csv:\n",
        "  reader = csv.reader(train_csv)\n",
        "  for row in reader:\n",
        "    questions.append(row[0])\n",
        "    answers.append(row[1])\n",
        "\n",
        "# Choose a random question and answer from the train.csv file\n",
        "i = random.randint(0, len(questions)-1)\n",
        "question = questions[i]\n",
        "dataset_answer = answers[i]\n",
        "\n",
        "# Print the chosen question\n",
        "print(\"Question:\", question)\n",
        "\n",
        "# Preprocess the user's answer and the answer in the training dataset\n",
        "user_answer = input(\"Enter your answer: \")\n",
        "vectorizer = CountVectorizer()\n",
        "X_user = vectorizer.fit_transform([user_answer])\n",
        "X_dataset = vectorizer.fit_transform([dataset_answer])\n",
        "\n",
        "\n",
        "\n",
        "X_shape = X_user.shape\n",
        "Y_shape = X_dataset.shape\n",
        "\n",
        "# If the number of columns in the X matrix is not equal to the number of columns in the Y matrix,\n",
        "# add or remove columns from one of the matrices as needed\n",
        "if X_shape[1] != Y_shape[1]:\n",
        "    # Calculate the difference in the number of columns\n",
        "    diff = Y_shape[1] - X_shape[1]\n",
        "    if diff > 0:\n",
        "        # Add columns to the X matrix\n",
        "        for i in range(diff):\n",
        "            X_user = np.append(X_user, 0)\n",
        "    else:\n",
        "        # Add columns to the Y matrix\n",
        "        for i in range(abs(diff)):\n",
        "            X_dataset = np.append(X_dataset, 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#X_user = X_user.todense()\n",
        "#X_dataset = X_dataset.todense()\n",
        "#X_user = csr_matrix(X_user).toarray()\n",
        "#X_dataset = csr_matrix(X_dataset).toarray()\n",
        "\n",
        "\n",
        "# Compute the cosine similarity between the user's answer and the answer in the training dataset\n",
        "similarity = cosine_similarity(X_user, X_dataset)\n",
        "score = similarity[0][0]\n",
        "\n",
        "# Print the similarity score\n",
        "print(\"Similarity score:\", score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qq8LNiV7RtK",
        "outputId": "8c65796f-983b-4a34-d593-1342ee326514"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: In speech recognition what kind of signal is used?\n",
            "Enter your answer: peech recognition acoustic signal used identify sequence word\n",
            "Similarity score: 0.9999999999999999\n"
          ]
        }
      ]
    }
  ]
}